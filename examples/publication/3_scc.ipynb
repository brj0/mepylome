{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "<img alt=\"Mepylome Logo\" src=\"https://raw.githubusercontent.com/brj0/mepylome/main/mepylome/data/assets/mepylome.svg\" width=\"300\">\n",
    "\n",
    "Mepylome: A Toolkit for DNA-Methylation Analysis in Tumor Diagnostics\n",
    "=====================================================================\n",
    "\n",
    "This notebook automates the analysis outlined in the Mepylome publication,\n",
    "performing all necessary steps from downloading datasets to executing the\n",
    "analyses.\n",
    "\n",
    "\n",
    "### Usage\n",
    "\n",
    "- Follow the notebook/script step-by-step.\n",
    "\n",
    "\n",
    "### System Tested\n",
    "\n",
    "- *Operating System*: Ubuntu 20.04.6\n",
    "- *Python Version*: 3.12\n",
    "\n",
    "\n",
    "### Reference Publication (will follow)\n",
    "\n",
    "- *Authors*: Jon Brugger et al.\n",
    "\n",
    "\n",
    "### Run This Notebook in Google Colab\n",
    "\n",
    "You can quickly open and run this notebook in Google Colab without any setup\n",
    "by clicking the link below.\n",
    "\n",
    "**Note**: The graphical user interface (GUI) features are limited in Google\n",
    "Colab. If using the free version, memory constraints may arise. Additionally,\n",
    "long download operations  may face timeouts or interruptions.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brj0/mepylome/blob/main/examples/publication/3_scc.ipynb)\n",
    "\n",
    "\n",
    "This notebook was automatically generated from the corresponding py-file\n",
    "with:\n",
    "\n",
    "```bash\n",
    "jupytext --to ipynb *.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "## Contents\n",
    "0. **[Initialization](#0.-Initialization)**\n",
    "1. **[Data Loading](#1.-Data-Loading)**\n",
    "2. **[UMAP Calculation](#2.-UMAP-Calculation)**\n",
    "3. **[Supervised Classifier Training](#3.-Supervised-Classifier-Training)**\n",
    "4. **[CNV Analysis](#4.-CNV-Analysis)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"0.-Initialization\"></a>\n",
    "## 0. Initialization\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "To run the analysis, install the following Python packages:\n",
    "- `mepylome` for DNA-methylation analysis\n",
    "- `ruptures` for segmentation in CNV plots\n",
    "- `ipython`, `pillow`, and `ipywidgets` for interactive and graphical\n",
    "  functionality\n",
    "- `kaleido` for saving plots\n",
    "\n",
    "Install them (1-2 minutes) using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install mepylome ruptures ipython pillow ipywidgets kaleido==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Core Imports, Configuration and main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import multiprocessing\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image\n",
    "\n",
    "from mepylome import ArrayType, Manifest, clear_cache, idat_basepaths\n",
    "from mepylome.analysis import MethylAnalysis\n",
    "from mepylome.dtypes.manifests import (\n",
    "    DOWNLOAD_DIR,\n",
    "    MANIFEST_URL,\n",
    "    REMOTE_FILENAME,\n",
    ")\n",
    "from mepylome.utils import download_file, download_idats\n",
    "\n",
    "# Define output font size for plots\n",
    "FONTSIZE = 23\n",
    "IMG_HEIGHT = 2000\n",
    "IMG_WIDTH = 1000\n",
    "GEO_URL = \"https://www.ncbi.nlm.nih.gov/geo/download/?acc={acc}&format=file\"\n",
    "\n",
    "# Determine basic storage directory depending on platform\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    # Google Colab\n",
    "    mepylome_dir = Path(\"/content/mepylome\")\n",
    "elif Path(\"/mnt/bender\").exists():\n",
    "    # Bender-specific path\n",
    "    mepylome_dir = Path(\"/mnt/bender/mepylome\")\n",
    "else:\n",
    "    # Default for local Linux or other environments\n",
    "    mepylome_dir = Path.home() / \"mepylome\"\n",
    "\n",
    "\n",
    "data_dir = mepylome_dir / \"data\"\n",
    "output_dir = mepylome_dir / \"outputs\"\n",
    "reference_dir = mepylome_dir / \"cnv_references\"\n",
    "validation_dir = mepylome_dir / \"validation_data\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "mepylome_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "validation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Number of CPUs: {multiprocessing.cpu_count()}\")\n",
    "print(f\"Data will be stored in: {mepylome_dir}\")\n",
    "\n",
    "\n",
    "# Main Functions\n",
    "\n",
    "\n",
    "def valid_idat_basenames(root_dir: Path) -> List[str]:\n",
    "    \"\"\"Returns all IDAT basenames that are of type 450k or epicv1.\"\"\"\n",
    "    all_idat_files = list(root_dir.rglob(\"*.idat*\"))\n",
    "    valid_array_types = {ArrayType.ILLUMINA_450K, ArrayType.ILLUMINA_EPIC}\n",
    "    invalid_idat_files = []\n",
    "    for idat_file in all_idat_files:\n",
    "        try:\n",
    "            array_type = ArrayType.from_idat(idat_file)\n",
    "        except Exception as e:\n",
    "            array_type = ArrayType.UNKNOWN\n",
    "        if array_type not in valid_array_types:\n",
    "            print(f\"Invalid file: {idat_file.name} (Type: {array_type})\")\n",
    "            invalid_idat_files.append(idat_file)\n",
    "    valid_basepaths = set(idat_basepaths(root_dir, only_valid=True)) - set(\n",
    "        idat_basepaths(invalid_idat_files)\n",
    "    )\n",
    "    return list(x.name for x in valid_basepaths)\n",
    "\n",
    "\n",
    "def extract_tar(tar_path: Path, output_directory: Path) -> None:\n",
    "    \"\"\"Extracts tar file under 'tar_path' to 'output_directory'.\"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        tar.extractall(path=output_directory, filter=\"data\")\n",
    "        print(f\"Extracted {tar_path} to {output_directory}\")\n",
    "\n",
    "\n",
    "def clean_filename(name: str):\n",
    "    \"\"\"Replace invalid characters with a single underscore.\"\"\"\n",
    "    return re.sub(r\"[^\\w\\-]+\", \"_\", name)\n",
    "\n",
    "\n",
    "def calculate_cn_summary(analysis: MethylAnalysis, class_: str) -> None:\n",
    "    \"\"\"Calculates and saves CN summary plots.\"\"\"\n",
    "    df_class = analysis.idat_handler.samples_annotated[class_]\n",
    "    plot_list = []\n",
    "    analysis_dir = analysis.analysis_dir\n",
    "    all_classes = sorted(df_class.unique())\n",
    "    for methyl_class in all_classes:\n",
    "        df_index = df_class == methyl_class\n",
    "        sample_ids = df_class.index[df_index]\n",
    "        plot, _ = analysis.cn_summary(sample_ids)\n",
    "        plot.update_layout(\n",
    "            title=f\"{methyl_class}\",\n",
    "            title_x=0.5,\n",
    "            yaxis_title=\"Proportion of CNV gains/losses\",\n",
    "        )\n",
    "        plot.update_layout(\n",
    "            title_font_size=FONTSIZE + 3,\n",
    "            yaxis_title_font_size=FONTSIZE - 2,\n",
    "        )\n",
    "        plot_list.append(plot)\n",
    "    png_paths = [\n",
    "        output_dir / f\"{analysis_dir.name}_cn_summary_{clean_filename(x)}.png\"\n",
    "        for x in all_classes\n",
    "    ]\n",
    "    for path, fig in zip(png_paths, plot_list):\n",
    "        fig.write_image(path)\n",
    "    images = [Image.open(path) for path in png_paths]\n",
    "    width, height = images[0].size\n",
    "    n_columns = 4\n",
    "    n_images = len(images)\n",
    "    n_rows = (n_images + n_columns - 1) // n_columns\n",
    "    total_width = width * n_columns\n",
    "    total_height = height * n_rows\n",
    "    new_image = Image.new(\"RGB\", (total_width, total_height), (255, 255, 255))\n",
    "    for index, img in enumerate(images):\n",
    "        row = index // n_columns\n",
    "        col = index % n_columns\n",
    "        x = col * width\n",
    "        y = row * height\n",
    "        new_image.paste(img, (x, y))\n",
    "    output_path = output_dir / f\"{analysis_dir.name}_cn_summary.png\"\n",
    "    new_image.save(output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Blacklist Generation for CpG Sites\n",
    "\n",
    "Some CpG sites should be excluded from the analysis. Here we choose probes\n",
    "flagged with `MFG_Change_Flagged` that should be excluded according to the\n",
    "manifest and those that are on sex chromosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blacklist_cpgs():\n",
    "    \"\"\"Returns and caches CpG sites that should be blacklisted.\"\"\"\n",
    "    print(\"Generating blacklist. Can take some time...\")\n",
    "    blacklist_path = data_dir / \"cpg_blacklist.csv\"\n",
    "    if not blacklist_path.exists():\n",
    "        manifest_url = MANIFEST_URL[ArrayType.ILLUMINA_EPIC]\n",
    "        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        response = requests.get(manifest_url)\n",
    "        html_sucess_ok_code = 200\n",
    "        if response.status_code == html_sucess_ok_code:\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as thezip:\n",
    "                thezip.extractall(DOWNLOAD_DIR)\n",
    "        else:\n",
    "            msg = f\"Failed to download the file: {response.status_code}\"\n",
    "            raise RuntimeError(msg)\n",
    "        csv_path = DOWNLOAD_DIR / REMOTE_FILENAME[ArrayType.ILLUMINA_EPIC]\n",
    "        manifest_df = pd.read_csv(csv_path, skiprows=7, low_memory=False)\n",
    "        flagged_cpgs = manifest_df[\n",
    "            manifest_df[\"MFG_Change_Flagged\"].fillna(False)\n",
    "        ][\"IlmnID\"]\n",
    "        flagged_cpgs.to_csv(blacklist_path, index=False, header=False)\n",
    "        csv_path.unlink()\n",
    "    blacklist_df = pd.read_csv(blacklist_path, header=None)\n",
    "    print(\"Generating blacklist done.\")\n",
    "    return set(blacklist_df.iloc[:, 0])\n",
    "\n",
    "\n",
    "def sex_chromosome_cpgs():\n",
    "    \"\"\"Returns CpGs on sex chromosomes for EPIC and 450k arrays.\"\"\"\n",
    "    manifest = Manifest(\"epic\")\n",
    "    sex_cpgs_epic = manifest.data_frame[\n",
    "        manifest.data_frame.Chromosome.isin([23, 24])\n",
    "    ].IlmnID\n",
    "    manifest = Manifest(\"450k\")\n",
    "    sex_cpgs_450k = manifest.data_frame[\n",
    "        manifest.data_frame.Chromosome.isin([23, 24])\n",
    "    ].IlmnID\n",
    "    return set(sex_cpgs_epic) | set(sex_cpgs_450k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### CNV-Neutral Reference Samples\n",
    "\n",
    "For generating copy number variation (CNV) plots, a sufficiently large set of\n",
    "CNV-neutral reference samples is required. Here, we use control samples from\n",
    "[Koelsche et al. (2021)](https://doi.org/10.1038/s41467-020-20603-4). These\n",
    "samples are stored in the designated reference_dir.\n",
    "\n",
    "**Best Practices**:\n",
    "- Include both fresh-frozen and FFPE (formalin-fixed paraffin-embedded)\n",
    "  samples in the copy-neutral reference set for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cn_neutral_samples = [\n",
    "    \"GSM4180453_201904410008_R06C01\",\n",
    "    \"GSM4180454_201904410008_R05C01\",\n",
    "    \"GSM4180455_201904410008_R04C01\",\n",
    "    \"GSM4180456_201904410008_R03C01\",\n",
    "    \"GSM4180457_201904410008_R02C01\",\n",
    "    \"GSM4180458_201904410008_R01C01\",\n",
    "    \"GSM4180459_201904410007_R08C01\",\n",
    "    \"GSM4180460_201904410007_R07C01\",\n",
    "    \"GSM4180741_201247480004_R05C01\",\n",
    "    \"GSM4180742_201247480004_R04C01\",\n",
    "    \"GSM4180743_201247480004_R03C01\",\n",
    "    \"GSM4180751_201194010006_R01C01\",\n",
    "    \"GSM4180909_200394870074_R04C02\",\n",
    "    \"GSM4180910_200394870074_R03C02\",\n",
    "    \"GSM4180911_200394870074_R02C02\",\n",
    "    \"GSM4180912_200394870074_R01C02\",\n",
    "    \"GSM4180913_200394870074_R05C01\",\n",
    "    \"GSM4180914_200394870074_R04C01\",\n",
    "    \"GSM4181456_203049640041_R03C01\",\n",
    "    \"GSM4181509_203049640040_R07C01\",\n",
    "    \"GSM4181510_203049640040_R08C01\",\n",
    "    \"GSM4181511_203049640041_R01C01\",\n",
    "    \"GSM4181512_203049640041_R02C01\",\n",
    "    \"GSM4181513_203049640041_R04C01\",\n",
    "    \"GSM4181514_203049640041_R05C01\",\n",
    "    \"GSM4181515_203049640041_R06C01\",\n",
    "    \"GSM4181516_203049640041_R07C01\",\n",
    "    \"GSM4181517_203049640041_R08C01\",\n",
    "]\n",
    "\n",
    "download_idats(dataset=cn_neutral_samples, save_dir=reference_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"1.-Data-Loading\"></a>\n",
    "## 1. Data Downloading\n",
    "\n",
    "In this example, we aim to reproduce the pan-SCC classifier presented in the\n",
    "study by [Jurmeister et al.\n",
    "(2019)](https://doi.org/10.1126/scitranslmed.aaw8513). Our goal is to gather\n",
    "data for Squamous Cell Carcinoma (SCC) from multiple sources, as outlined in\n",
    "the publication, including datasets from The Cancer Genome Atlas (TCGA) and\n",
    "from the Gene Expression Omnibus (GEO) repository. Datasets without IDAT\n",
    "files are omitted from the collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Step 1: Download and unzip metadata for TCGA files and GEO series.\n",
    "The GEO annotation files were downloaded and manually curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories.\n",
    "tumor_site = \"scc\"\n",
    "analysis_dir = data_dir / tumor_site\n",
    "test_dir = validation_dir / tumor_site\n",
    "\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download TCGA metadata\n",
    "tcga_metadata_dir_tar = analysis_dir / \"tcga_metadata.tar.gz\"\n",
    "tcga_metadata_dir = tcga_metadata_dir_tar.with_suffix(\"\").with_suffix(\"\")\n",
    "tcga_metadata_url = \"https://raw.githubusercontent.com/brj0/mepylome-data/main/examples/tcga_metadata.tar.gz\"\n",
    "\n",
    "# Check if the TCGA annotation tar file exists and extract\n",
    "if not tcga_metadata_dir.exists():\n",
    "    print(\"Setting up TCGA annotation directory...\")\n",
    "    download_file(tcga_metadata_url, tcga_metadata_dir_tar)\n",
    "    extract_tar(tcga_metadata_dir_tar, analysis_dir)\n",
    "    print(\"Setting up TCGA annotation directory done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Download GEO metadata\n",
    "geo_metadata_dir_tar = analysis_dir / \"geo_metadata.tar.gz\"\n",
    "geo_metadata_dir = geo_metadata_dir_tar.with_suffix(\"\").with_suffix(\"\")\n",
    "geo_metadata_url = \"https://raw.githubusercontent.com/brj0/mepylome-data/main/examples/geo_metadata.tar.gz\"\n",
    "\n",
    "# Check if the GEO annotation tar file exists and extract\n",
    "if not geo_metadata_dir.exists():\n",
    "    print(\"Setting up GEO annotation directory...\")\n",
    "    download_file(geo_metadata_url, geo_metadata_dir_tar)\n",
    "    extract_tar(geo_metadata_dir_tar, analysis_dir)\n",
    "    print(\"Setting up GEO annotation directory done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Step 2: Download all IDAT files.\n",
    "\n",
    "We download all SCC samples from TCGA that contain IDAT files. **This may\n",
    "take several hours.** Then we download GEO IDAT files.\n",
    "\n",
    "**Important:** Downloading from TCGA can be unreliable. Connection resets and\n",
    "server-side interruptions are common.\n",
    "\n",
    "This script is designed to be resilient: it automatically **retries failed\n",
    "downloads** up to 5 times. After the final attempt, it will continue\n",
    "regardless of whether some files failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Metadata for TCGA download\n",
    "metadata_cart = next(tcga_metadata_dir.glob(\"metadata.cart.*json\"))\n",
    "metadata_clinical = tcga_metadata_dir / \"clinical.tsv\"\n",
    "\n",
    "# Define dataset to download both for training and validation data.\n",
    "datasets = {\n",
    "    \"scc\": {\n",
    "        \"xlsx\": \"https://www.science.org/doi/suppl/10.1126/scitranslmed.aaw8513/suppl_file/aaw8513_data_file_s1.xlsx\",\n",
    "        \"idat\": [\n",
    "            {\n",
    "                \"source\": \"tcga\",\n",
    "                \"metadata_cart\": metadata_cart,\n",
    "                \"metadata_clinical\": metadata_clinical,\n",
    "                \"subdir\": \"tcga_scc\",\n",
    "                \"meta\": \"tcga_annotation\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    \"scc_test\": {\n",
    "        \"idat\": [\n",
    "            \"GSE124052\",  # HNSQ_CA, NSCLC_SC\n",
    "            \"GSE66836\",  # NSCLC_AD, CONTR_LUNG\n",
    "            \"GSE79556\",  # HNSQ_CA (oral tongue)\n",
    "            \"GSE87053\",  # HNSQ_CA, CONTR_OC\n",
    "            \"GSE95036\",  # HNSQ_CA\n",
    "            \"GSE124052\",  # HNSQ_CA, NSCLC_SC\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Download the IDAT files.\n",
    "if not (download_completed := data_dir / \"download_completed.txt\").exists():\n",
    "    print(\"Downloading IDAT files. This may take several hours!\")\n",
    "    download_idats(dataset=datasets[\"scc\"][\"idat\"], save_dir=analysis_dir)\n",
    "    download_idats(dataset=datasets[\"scc_test\"][\"idat\"], save_dir=test_dir)\n",
    "    download_completed.touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 3: Tidy up and merge annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Rename columns restrict to the useful ones\n",
    "def tidy_up_tcga_annotation(annotation_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and standardize TCGA annotation DataFrame.\"\"\"\n",
    "    annotation_df = annotation_df.copy()\n",
    "    columns_dict = {\n",
    "        \"gender\": \"Sex\",\n",
    "        \"age_at_index\": \"Age\",\n",
    "        \"tissue_or_organ_of_origin\": \"Tumor_site\",\n",
    "        \"site_of_resection_or_biopsy\": \"Site_of_resection_or_biopsy\",\n",
    "        \"tumor_grade\": \"Tumor_grade\",\n",
    "        \"morphology\": \"Morphology\",\n",
    "        \"primary_diagnosis\": \"Primary_diagnosis\",\n",
    "        \"Sample_ID\": \"Sample_ID\",\n",
    "    }\n",
    "    annotation_df = annotation_df.rename(columns=columns_dict)\n",
    "    annotation_df = annotation_df[columns_dict.values()].copy()\n",
    "    # Standardize the 'Sex' column and convert 'Age' to numeric\n",
    "    annotation_df[\"Sex\"] = annotation_df[\"Sex\"].replace(\n",
    "        {\"female\": \"Female\", \"male\": \"Male\"}\n",
    "    )\n",
    "    annotation_df[\"Age\"] = pd.to_numeric(annotation_df[\"Age\"], errors=\"coerce\")\n",
    "    # Mark the samples that to be censored\n",
    "    diag_to_censor_stat = {\n",
    "        \"Adenocarcinoma with mixed subtypes\": 0,\n",
    "        \"Adenocarcinoma, NOS\": 0,\n",
    "        \"Adenosquamous carcinoma\": 1,\n",
    "        \"Basaloid squamous cell carcinoma\": 1,\n",
    "        \"Lymphoepithelial carcinoma\": 1,\n",
    "        \"Papillary carcinoma, NOS\": 1,\n",
    "        \"Papillary squamous cell carcinoma\": 1,\n",
    "        \"Squamous cell carcinoma, NOS\": 0,\n",
    "        \"Squamous cell carcinoma, keratinizing, NOS\": 0,\n",
    "        \"Squamous cell carcinoma, large cell, nonkeratinizing, NOS\": 1,\n",
    "        \"Squamous cell carcinoma, nonkeratinizing, NOS\": 0,\n",
    "        \"Squamous cell carcinoma, small cell, nonkeratinizing\": 1,\n",
    "        \"Squamous cell carcinoma, spindle cell\": 1,\n",
    "        \"Warty carcinoma\": 1,\n",
    "    }\n",
    "    annotation_df[\"Censor\"] = annotation_df[\"Primary_diagnosis\"].map(\n",
    "        diag_to_censor_stat\n",
    "    )\n",
    "    # Condense the primary tumor site.\n",
    "    nsclc_sites = {\n",
    "        \"Lower lobe, lung\",\n",
    "        \"Lung, NOS\",\n",
    "        \"Main bronchus\",\n",
    "        \"Middle lobe, lung\",\n",
    "        \"Overlapping lesion of lung\",\n",
    "        \"Upper lobe, lung\",\n",
    "    }\n",
    "    hnsq_sites = {\n",
    "        \"Anterior floor of mouth\",\n",
    "        \"Base of tongue, NOS\",\n",
    "        \"Border of tongue\",\n",
    "        \"Cheek mucosa\",\n",
    "        \"Floor of mouth, NOS\",\n",
    "        \"Gum, NOS\",\n",
    "        \"Hard palate\",\n",
    "        \"Head, face or neck, NOS\",\n",
    "        \"Hypopharynx, NOS\",\n",
    "        \"Larynx, NOS\",\n",
    "        \"Lip, NOS\",\n",
    "        \"Lower gum\",\n",
    "        \"Mandible\",\n",
    "        \"Mouth, NOS\",\n",
    "        \"Nasal cavity\",\n",
    "        \"Oropharynx, NOS\",\n",
    "        \"Overlapping lesion of lip, oral cavity and pharynx\",\n",
    "        \"Palate, NOS\",\n",
    "        \"Pharynx, NOS\",\n",
    "        \"Posterior wall of oropharynx\",\n",
    "        \"Retromolar area\",\n",
    "        \"Supraglottis\",\n",
    "        \"Tongue, NOS\",\n",
    "        \"Tonsil, NOS\",\n",
    "        \"Upper Gum\",\n",
    "        \"Ventral surface of tongue, NOS\",\n",
    "    }\n",
    "    cervix_sites = {\"Cervix uteri\"}\n",
    "    eso_sites = {\n",
    "        \"Cardia, NOS\",\n",
    "        \"Esophagus, NOS\",\n",
    "        \"Lower third of esophagus\",\n",
    "        \"Middle third of esophagus\",\n",
    "        \"Thoracic esophagus\",\n",
    "        \"Upper third of esophagus\",\n",
    "    }\n",
    "    annotation_df[\"Diagnosis\"] = None\n",
    "    # Classify each row based on the tumor site\n",
    "    for index, row in annotation_df.iterrows():\n",
    "        site = str(row[\"Tumor_site\"]).strip()\n",
    "        diagnosis = row[\"Primary_diagnosis\"]\n",
    "        if diagnosis.startswith(\"Adenocarcinoma\") and site in nsclc_sites:\n",
    "            annotation_df.loc[index, \"Diagnosis\"] = \"NSCLC_AD\"\n",
    "        elif site in cervix_sites:\n",
    "            annotation_df.loc[index, \"Diagnosis\"] = \"CERSQ_CA\"\n",
    "        elif site in nsclc_sites:\n",
    "            annotation_df.loc[index, \"Diagnosis\"] = \"NSCLC_SC\"\n",
    "        elif site in hnsq_sites:\n",
    "            annotation_df.loc[index, \"Diagnosis\"] = \"HNSQ_CA\"\n",
    "        elif site in eso_sites:\n",
    "            annotation_df.loc[index, \"Diagnosis\"] = \"ESO_CA_SQ\"\n",
    "        else:\n",
    "            annotation_df.loc[index, \"Censor\"] = 1\n",
    "            print(f\"Unmatched tumor site: {site} (index {index} - censored)\")\n",
    "    # Removed censored samples\n",
    "    return annotation_df[annotation_df[\"Censor\"] == 0]\n",
    "\n",
    "\n",
    "tcga_annotation_path = next(analysis_dir.rglob(\"tcga_annotation.csv\"))\n",
    "tcga_annotation = pd.read_csv(tcga_annotation_path)\n",
    "tcga_annotation = tidy_up_tcga_annotation(tcga_annotation)\n",
    "\n",
    "\n",
    "# Merge the annotation spreadsheets\n",
    "def merge_csv(dir_path):\n",
    "    \"\"\"Reads all CSV files merges them.\"\"\"\n",
    "    dir_path = Path(dir_path)\n",
    "    merged_df = pd.DataFrame()\n",
    "    for csv_file in dir_path.glob(\"*.csv\"):\n",
    "        print(f\"Reading {csv_file}\")\n",
    "        data_frame = pd.read_csv(csv_file)\n",
    "        merged_df = pd.concat([merged_df, data_frame], ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Join the TCGA and GEO annotation files\n",
    "if (csv_path := analysis_dir / f\"{tumor_site}.csv\").exists():\n",
    "    anno_df = pd.read_csv(csv_path)\n",
    "    print(\"Merged annotation file allready exists.\")\n",
    "else:\n",
    "    geo_annotation = merge_csv(geo_metadata_dir)\n",
    "    anno_df = pd.concat([geo_annotation, tcga_annotation], ignore_index=True)\n",
    "    # Remove Adenocarcinoma and normal oral cavity samples\n",
    "    scc_types = {\"HNSQ_CA\", \"NSCLC_SC\", \"ESO_CA_SQ\", \"CERSQ_CA\"}\n",
    "    anno_df = anno_df[anno_df[\"Diagnosis\"].isin(scc_types)]\n",
    "    anno_df.to_csv(csv_path, index=False)\n",
    "    print(\"Merged annotation file created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Create the Methylation Analysis Object\n",
    "\n",
    "The `MethylAnalysis` object serves as the main interface for performing DNA\n",
    "methylation analysis. Key parameters such as the directory structure, number\n",
    "of CpG sites, and UMAP settings are configured here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider test files with annotation.\n",
    "test_ids = set(anno_df.Sample_ID).intersection(\n",
    "    x.name for x in idat_basepaths(test_dir, only_valid=True)\n",
    ")\n",
    "\n",
    "# Only consider valid 450k and epic files\n",
    "analysis_ids = valid_idat_basenames(analysis_dir)\n",
    "\n",
    "# Choose CpG list that should be blacklisted\n",
    "blacklist = generate_blacklist_cpgs() | sex_chromosome_cpgs()\n",
    "\n",
    "\n",
    "analysis = MethylAnalysis(\n",
    "    analysis_dir=analysis_dir,\n",
    "    analysis_ids=analysis_ids,\n",
    "    reference_dir=reference_dir,\n",
    "    output_dir=output_dir,\n",
    "    test_dir=test_dir,\n",
    "    test_ids=test_ids,\n",
    "    n_cpgs=25000,\n",
    "    load_full_betas=True,\n",
    "    overlap=True,\n",
    "    cpg_blacklist=blacklist,\n",
    "    cpgs=\"450k+epic+epicv2\",\n",
    "    debug=False,\n",
    "    do_seg=True,\n",
    "    umap_parms={\n",
    "        \"n_neighbors\": 5,\n",
    "        \"metric\": \"manhattan\",\n",
    "        \"min_dist\": 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Load Beta Values\n",
    "\n",
    "Reads and processes beta values from the provided dataset. This step is\n",
    "optional and primarily demonstrates the time required for processing. If not\n",
    "performed here, it will be automatically executed in the background when\n",
    "needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis.set_betas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"2.-UMAP-Calculation\"></a>\n",
    "## 2. UMAP Calculation\n",
    "\n",
    "### Generate UMAP Plot\n",
    "\n",
    "Set the columns used for coloring the UMAP plot before initiating the\n",
    "dimensionality reduction process. The UMAP algorithm produces a visual\n",
    "representation of the sample clusters, which is stored as a Plotly object in\n",
    "`analysis.umap_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate UMAP\n",
    "analysis.idat_handler.selected_columns = [\"Diagnosis\"]\n",
    "analysis.make_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "print(analysis.umap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate and show image\n",
    "output_path = output_dir / f\"{analysis_dir.name}_umap_plot.jpg\"\n",
    "analysis.umap_plot.write_image(\n",
    "    output_path,\n",
    "    format=\"jpg\",\n",
    "    width=IMG_HEIGHT,\n",
    "    height=IMG_WIDTH,\n",
    "    scale=1,\n",
    ")\n",
    "IPImage(filename=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Launch the Analysis GUI\n",
    "\n",
    "Initializes an interactive GUI for further exploration of the methylation\n",
    "data.\n",
    "\n",
    "**Note:** This step works best in local environments and may have limitations\n",
    "on platforms like Google Colab or Binder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis.run_app(open_tab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "On memory-limited platforms such as Google Colab, we need to manually free up\n",
    "memory between operations to avoid crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"3.-Supervised-Classifier-Training\"></a>\n",
    "## 3. Supervised Classifier Training\n",
    "\n",
    "### Supervised Classifier Validation\n",
    "\n",
    "The next step involves validating various supervised classification\n",
    "algorithms to evaluate their performance on the dataset. This process helps\n",
    "identify the most accurate model for methylation-based classification.\n",
    "\n",
    "**Note**:\n",
    "Training is resource- and time-intensive. The process may take up to 10\n",
    "minutes, depending on the computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "***Preselect features (optional, recommended for low-memory systems)***:\n",
    "\n",
    "To optimize memory usage, we use a custom `feature_matrix` for training\n",
    "instead of the full `betas_all`, which includes all CpG sites. When the beta\n",
    "values are selected via `set_betas`, both `betas_sel` and `betas_all` are\n",
    "produced. While `betas_all` contains all CpGs, `betas_sel` (in our case)\n",
    "includes the top 25,000 most variable CpG sites.\n",
    "\n",
    "This step is crucial for low-memory systems (e.g., Google Colab) to reduce\n",
    "memory consumption. If memory is not a concern, you can skip this step and\n",
    "include all CpGs in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Preselect top 25,000 UMAP CpGs to reduce memory usage (optional)\n",
    "analysis.feature_matrix = analysis.betas_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train supervised classifiers\n",
    "\n",
    "analysis.idat_handler.selected_columns = [\"Diagnosis\"]\n",
    "ids = analysis.idat_handler.ids\n",
    "clf_out = analysis.classify(\n",
    "    ids=ids,\n",
    "    clf_list=[\n",
    "        \"vtl-kbest(k=10000)-et\",\n",
    "        \"vtl-kbest(k=10000)-lr(max_iter=10000)\",\n",
    "        \"vtl-kbest(k=10000)-rf\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print reports for all classifier for the first sample\n",
    "for clf_result in clf_out:\n",
    "    print(clf_result.reports[\"txt\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and display the best classifier\n",
    "best_clf = max(\n",
    "    clf_out, key=lambda result: np.mean(result.metrics[\"accuracy_scores\"])\n",
    ")\n",
    "print(\"Most accurate classifier:\")\n",
    "print(best_clf.reports[\"txt\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Now we apply the best classifier on the independent validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = analysis.idat_handler.test_ids\n",
    "# Ignore all files that are not in annotation file\n",
    "test_ids = list(\n",
    "    set(test_ids).intersection(analysis.idat_handler.annotation_df.index)\n",
    ")\n",
    "clf_out_pred = analysis.classify(ids=test_ids, clf_list=best_clf.model)\n",
    "pred = clf_out_pred[0].prediction.idxmax(axis=1)\n",
    "true_values = analysis.idat_handler.samples_annotated.loc[test_ids][\n",
    "    \"Diagnosis\"\n",
    "]\n",
    "correct = np.sum(pred == true_values)\n",
    "total = len(pred)\n",
    "accuracy = correct / total\n",
    "print(f\"Classifier Accuracy: {100 * accuracy:.2f} % ({correct}/{total})\")\n",
    "\n",
    "# Analyze misclassified samples\n",
    "misclassified = pred != true_values\n",
    "misclassified_samples = clf_out_pred[0].prediction[misclassified].copy()\n",
    "misclassified_samples[\"Pred\"] = pred[misclassified]\n",
    "misclassified_samples[\"True\"] = true_values[misclassified]\n",
    "print(\"Missclassified samples:\\n\", misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"4.-CNV-Analysis\"></a>\n",
    "## 4. CNV Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Generate and Save CNV Plot\n",
    "\n",
    "Creates a copy number variation (CNV) plot for a specified sample and saves\n",
    "the output as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CNV example\n",
    "analysis.make_cnv_plot(\"GSM3519735_202915460126_R05C01\")\n",
    "cnv_plot = analysis.cnv_plot\n",
    "cnv_plot.update_layout(\n",
    "    yaxis_range=[-1.1, 1.1],\n",
    "    font={\"size\": FONTSIZE},\n",
    "    margin={\"t\": 50},\n",
    ")\n",
    "output_path = output_dir / f\"{analysis_dir.name}_cnv_plot.jpg\"\n",
    "cnv_plot.write_image(\n",
    "    output_path,\n",
    "    format=\"jpg\",\n",
    "    width=IMG_HEIGHT,\n",
    "    height=IMG_WIDTH,\n",
    "    scale=1,\n",
    ")\n",
    "IPImage(filename=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Generate CNV Summary Plots\n",
    "\n",
    "In addition to individual CNV plots, this step computes summary plots to\n",
    "highlight genomic alterations across multiple samples.\n",
    "\n",
    "**Note**:\n",
    "Generating all copy number variation (CNV) plots is resource- and\n",
    "time-intensive. The process can take a significant amount of time, depending\n",
    "on the computational resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.precompute_cnvs()\n",
    "cn_summary_path = calculate_cn_summary(analysis, \"Diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPImage(filename=cn_summary_path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
