{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "<img alt=\"Mepylome Logo\" src=\"https://raw.githubusercontent.com/brj0/mepylome/main/mepylome/data/assets/mepylome.svg\" width=\"300\">\n",
    "\n",
    "Mepylome: A Toolkit for DNA-Methylation Analysis in Tumor Diagnostics\n",
    "=====================================================================\n",
    "\n",
    "This notebook automates the analysis outlined in the Mepylome publication,\n",
    "performing all necessary steps from downloading datasets to executing the\n",
    "analyses.\n",
    "\n",
    "\n",
    "### Usage\n",
    "\n",
    "- Follow the notebook/script step-by-step.\n",
    "\n",
    "\n",
    "### System Tested\n",
    "\n",
    "- *Operating System*: Ubuntu 20.04.6\n",
    "- *Python Version*: 3.12\n",
    "\n",
    "\n",
    "### Reference Publication (will follow)\n",
    "\n",
    "- *Title*: Mepylome: A User-Friendly Open-Source Toolkit for DNA-Methylation\n",
    "  Analysis in Tumor Diagnostics\n",
    "- *Author*: Jon Brugger et al.\n",
    "\n",
    "\n",
    "### Run This Notebook in Google Colab\n",
    "\n",
    "You can quickly open and run this notebook in Google Colab without any setup\n",
    "by clicking the link below.\n",
    "\n",
    "**Note**: The graphical user interface (GUI) features are limited in Google\n",
    "Colab. If using the free version, memory constraints may arise. Additionally,\n",
    "long download operations  may face timeouts or interruptions.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brj0/mepylome/blob/main/examples/publication/scc.ipynb)\n",
    "\n",
    "\n",
    "This notebook was automatically generated from the corresponding py-file\n",
    "with:\n",
    "\n",
    "```bash\n",
    "jupytext --to ipynb *.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "## Contents\n",
    "0. **[Initialization](#0.-Initialization)**\n",
    "1. **[Data Loading](#1.-Data-Loading)**\n",
    "2. **[UMAP Calculation](#2.-UMAP-Calculation)**\n",
    "3. **[Supervised Classifier Training](#3.-Supervised-Classifier-Training)**\n",
    "4. **[CNV Analysis](#4.-CNV-Analysis)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"0.-Initialization\"></a>\n",
    "## 0. Initialization\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "To run the analysis, install the following Python packages:\n",
    "- `mepylome` - the main toolkit for DNA-methylation analysis.\n",
    "- `ruptures` - used for segmentation calculations in CNV plots.\n",
    "- `kaleido` for saving plots.\n",
    "- `ipython` and `pillow` - supporting libraries for interactive and graphical\n",
    "  functionality.\n",
    "\n",
    "\n",
    "Install these packages (may take 1 to 2 minutes) using the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install mepylome ipython pillow ruptures ipywidgets\n",
    "pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Core Imports, Configuration and main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile\n",
    "from concurrent.futures import (\n",
    "    ThreadPoolExecutor,\n",
    "    as_completed,\n",
    ")\n",
    "from pathlib import Path\n",
    "from threading import Lock\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image\n",
    "\n",
    "from mepylome import ArrayType, Manifest, clear_cache, idat_basepaths\n",
    "from mepylome.analysis import MethylAnalysis\n",
    "from mepylome.dtypes.manifests import (\n",
    "    DOWNLOAD_DIR,\n",
    "    MANIFEST_URL,\n",
    "    REMOTE_FILENAME,\n",
    ")\n",
    "from mepylome.utils.files import (\n",
    "    download_file,\n",
    "    download_geo_probes,\n",
    ")\n",
    "\n",
    "# Define output font size for plots\n",
    "FONTSIZE = 23\n",
    "IMG_HEIGHT = 2000\n",
    "IMG_WIDTH = 1000\n",
    "GEO_URL = \"https://www.ncbi.nlm.nih.gov/geo/download/?acc={acc}&format=file\"\n",
    "\n",
    "# Define dataset URLs and filenames\n",
    "datasets = {\n",
    "    \"scc\": {\n",
    "        \"xlsx\": \"https://www.science.org/doi/suppl/10.1126/scitranslmed.aaw8513/suppl_file/aaw8513_data_file_s1.xlsx\",\n",
    "        \"geo_ids\": [],\n",
    "    },\n",
    "    \"scc_test\": {\n",
    "        \"geo_ids\": [\n",
    "            \"GSE124052\",  # HNSQ_CA, NSCLC_SC\n",
    "            \"GSE66836\",  # NSCLC_AD, CONTR_LUNG\n",
    "            \"GSE79556\",  # HNSQ_CA (oral tongue)\n",
    "            \"GSE87053\",  # HNSQ_CA, CONTR_OC\n",
    "            \"GSE95036\",  # HNSQ_CA\n",
    "            \"GSE124052\",  # HNSQ_CA, NSCLC_SC\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Determine basic storage directory depending on platform\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    # Google Colab\n",
    "    mepylome_dir = Path(\"/content/mepylome\")\n",
    "elif Path(\"/mnt/bender\").exists():\n",
    "    # Bender-specific path\n",
    "    mepylome_dir = Path(\"/mnt/bender/mepylome\")\n",
    "else:\n",
    "    # Default for local Linux or other environments\n",
    "    mepylome_dir = Path.home() / \"mepylome\"\n",
    "\n",
    "\n",
    "data_dir = mepylome_dir / \"data\"\n",
    "output_dir = mepylome_dir / \"outputs\"\n",
    "reference_dir = mepylome_dir / \"cnv_references\"\n",
    "validation_dir = mepylome_dir / \"validation_data\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "mepylome_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "validation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Number of CPUs: {multiprocessing.cpu_count()}\")\n",
    "print(f\"Data will be stored in: {mepylome_dir}\")\n",
    "\n",
    "\n",
    "# Main Functions\n",
    "\n",
    "\n",
    "def extract_tar(tar_path, output_directory):\n",
    "    \"\"\"Extracts tar file under 'tar_path' to 'output_directory'.\"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        tar.extractall(path=output_directory, filter=\"data\")\n",
    "        print(f\"Extracted {tar_path} to {output_directory}\")\n",
    "\n",
    "\n",
    "def download_from_geo_and_untar(analysis_dir, geo_ids):\n",
    "    \"\"\"Downloads all missing GEO files and untars them.\"\"\"\n",
    "    for geo_id in geo_ids:\n",
    "        idat_dir = analysis_dir / geo_id\n",
    "        if idat_dir.exists():\n",
    "            print(f\"Data for GEO ID {geo_id} already exists, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            tar_path = analysis_dir / f\"{geo_id}.tar\"\n",
    "            geo_url = GEO_URL.format(acc=geo_id)\n",
    "            download_file(geo_url, tar_path)\n",
    "            extract_tar(tar_path, idat_dir)\n",
    "            tar_path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing GEO ID {geo_id}: {e}\")\n",
    "\n",
    "\n",
    "def clean_filename(name):\n",
    "    \"\"\"Replace invalid characters with a single underscore.\"\"\"\n",
    "    return re.sub(r\"[^\\w\\-]+\", \"_\", name)\n",
    "\n",
    "\n",
    "def calculate_cn_summary(analysis, class_):\n",
    "    \"\"\"Calculates and saves CN summary plots.\"\"\"\n",
    "    df_class = analysis.idat_handler.samples_annotated[class_]\n",
    "    plot_list = []\n",
    "    analysis_dir = analysis.analysis_dir\n",
    "    all_classes = sorted(df_class.unique())\n",
    "    for methyl_class in all_classes:\n",
    "        df_index = df_class == methyl_class\n",
    "        sample_ids = df_class.index[df_index]\n",
    "        plot, _ = analysis.cn_summary(sample_ids)\n",
    "        plot.update_layout(\n",
    "            title=f\"{methyl_class}\",\n",
    "            title_x=0.5,\n",
    "            yaxis_title=\"Proportion of CNV gains/losses\",\n",
    "        )\n",
    "        plot.update_layout(\n",
    "            title_font_size=FONTSIZE + 3,\n",
    "            yaxis_title_font_size=FONTSIZE - 2,\n",
    "        )\n",
    "        plot_list.append(plot)\n",
    "    png_paths = [\n",
    "        output_dir / f\"{analysis_dir.name}_cn_summary_{clean_filename(x)}.png\"\n",
    "        for x in all_classes\n",
    "    ]\n",
    "    for path, fig in zip(png_paths, plot_list):\n",
    "        fig.write_image(path)\n",
    "    images = [Image.open(path) for path in png_paths]\n",
    "    width, height = images[0].size\n",
    "    n_columns = 4\n",
    "    n_images = len(images)\n",
    "    n_rows = (n_images + n_columns - 1) // n_columns\n",
    "    total_width = width * n_columns\n",
    "    total_height = height * n_rows\n",
    "    new_image = Image.new(\"RGB\", (total_width, total_height), (255, 255, 255))\n",
    "    for index, img in enumerate(images):\n",
    "        row = index // n_columns\n",
    "        col = index % n_columns\n",
    "        x = col * width\n",
    "        y = row * height\n",
    "        new_image.paste(img, (x, y))\n",
    "    output_path = output_dir / f\"{analysis_dir.name}_cn_summary.png\"\n",
    "    new_image.save(output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Blacklist Generation for CpG Sites\n",
    "\n",
    "Some CpG sites should be excluded from the analysis. Here we choose probes\n",
    "flagged with `MFG_Change_Flagged` that should be excluded according to the\n",
    "manifest and those that are on sex chromosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blacklist_cpgs():\n",
    "    \"\"\"Returns and caches CpG sites that should be blacklisted.\"\"\"\n",
    "    print(\"Generating blacklist. Can take some time...\")\n",
    "    blacklist_path = data_dir / \"cpg_blacklist.csv\"\n",
    "    if not blacklist_path.exists():\n",
    "        manifest_url = MANIFEST_URL[ArrayType.ILLUMINA_EPIC]\n",
    "        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        response = requests.get(manifest_url)\n",
    "        html_sucess_ok_code = 200\n",
    "        if response.status_code == html_sucess_ok_code:\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as thezip:\n",
    "                thezip.extractall(DOWNLOAD_DIR)\n",
    "        else:\n",
    "            msg = f\"Failed to download the file: {response.status_code}\"\n",
    "            raise RuntimeError(msg)\n",
    "        csv_path = DOWNLOAD_DIR / REMOTE_FILENAME[ArrayType.ILLUMINA_EPIC]\n",
    "        manifest_df = pd.read_csv(csv_path, skiprows=7, low_memory=False)\n",
    "        flagged_cpgs = manifest_df[\n",
    "            manifest_df[\"MFG_Change_Flagged\"].fillna(False)\n",
    "        ][\"IlmnID\"]\n",
    "        flagged_cpgs.to_csv(blacklist_path, index=False, header=False)\n",
    "        csv_path.unlink()\n",
    "    blacklist_df = pd.read_csv(blacklist_path, header=None)\n",
    "    print(\"Generating blacklist done.\")\n",
    "    return set(blacklist_df.iloc[:, 0])\n",
    "\n",
    "\n",
    "def sex_chromosome_cpgs():\n",
    "    \"\"\"Returns CpGs on sex chromosomes for EPIC and 450k arrays.\"\"\"\n",
    "    manifest = Manifest(\"epic\")\n",
    "    sex_cpgs_epic = manifest.data_frame[\n",
    "        manifest.data_frame.Chromosome.isin([23, 24])\n",
    "    ].IlmnID\n",
    "    manifest = Manifest(\"450k\")\n",
    "    sex_cpgs_450k = manifest.data_frame[\n",
    "        manifest.data_frame.Chromosome.isin([23, 24])\n",
    "    ].IlmnID\n",
    "    return set(sex_cpgs_epic) | set(sex_cpgs_450k)\n",
    "\n",
    "\n",
    "# Choose CpG list that should be blacklisted\n",
    "blacklist = generate_blacklist_cpgs() | sex_chromosome_cpgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### CNV-Neutral Reference Samples\n",
    "\n",
    "To ensure accurate analysis, we utilize control probes from [Koelsche et\n",
    "al. (2021)](https://doi.org/10.1038/s41467-020-20603-4). These probes\n",
    "are stored in the designated reference directory `reference_dir`.\n",
    "\n",
    "**Best Practices**:\n",
    "- Include both fresh-frozen and FFPE (formalin-fixed paraffin-embedded)\n",
    "  samples in the copy-neutral reference set for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cn_neutral_probes = [\n",
    "    \"GSM4180453_201904410008_R06C01\",\n",
    "    \"GSM4180454_201904410008_R05C01\",\n",
    "    \"GSM4180455_201904410008_R04C01\",\n",
    "    \"GSM4180456_201904410008_R03C01\",\n",
    "    \"GSM4180457_201904410008_R02C01\",\n",
    "    \"GSM4180458_201904410008_R01C01\",\n",
    "    \"GSM4180459_201904410007_R08C01\",\n",
    "    \"GSM4180460_201904410007_R07C01\",\n",
    "    \"GSM4180741_201247480004_R05C01\",\n",
    "    \"GSM4180742_201247480004_R04C01\",\n",
    "    \"GSM4180743_201247480004_R03C01\",\n",
    "    \"GSM4180751_201194010006_R01C01\",\n",
    "    \"GSM4180909_200394870074_R04C02\",\n",
    "    \"GSM4180910_200394870074_R03C02\",\n",
    "    \"GSM4180911_200394870074_R02C02\",\n",
    "    \"GSM4180912_200394870074_R01C02\",\n",
    "    \"GSM4180913_200394870074_R05C01\",\n",
    "    \"GSM4180914_200394870074_R04C01\",\n",
    "    \"GSM4181456_203049640041_R03C01\",\n",
    "    \"GSM4181509_203049640040_R07C01\",\n",
    "    \"GSM4181510_203049640040_R08C01\",\n",
    "    \"GSM4181511_203049640041_R01C01\",\n",
    "    \"GSM4181512_203049640041_R02C01\",\n",
    "    \"GSM4181513_203049640041_R04C01\",\n",
    "    \"GSM4181514_203049640041_R05C01\",\n",
    "    \"GSM4181515_203049640041_R06C01\",\n",
    "    \"GSM4181516_203049640041_R07C01\",\n",
    "    \"GSM4181517_203049640041_R08C01\",\n",
    "]\n",
    "\n",
    "download_geo_probes(reference_dir, cn_neutral_probes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"1.-Data-Loading\"></a>\n",
    "## 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "title": "In this example, we aim to reproduce the pan-SCC classifier"
   },
   "source": [
    "presented in the study by [Jurmeister et al.\n",
    "(2019)](https://doi.org/10.1126/scitranslmed.aaw8513). Our goal is to gather\n",
    "data for Squamous Cell Carcinoma (SCC) from multiple sources, as outlined in\n",
    "the publication, including datasets from The Cancer Genome Atlas (TCGA) and\n",
    "from the Gene Expression Omnibus (GEO) repository. Datasets without IDAT\n",
    "files are omitted from the collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories.\n",
    "tumor_site = \"scc\"\n",
    "analysis_dir = data_dir / tumor_site\n",
    "test_dir = validation_dir / tumor_site\n",
    "\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 1: Download TCGA Data\n",
    "\n",
    "We download the complete TCGA dataset. **This may take several hours.**\n",
    "\n",
    "**Important:** Downloading from TCGA can be unreliable. Connection resets and\n",
    "server-side interruptions are common.\n",
    "\n",
    "This script is designed to be resilient: it automatically **retries failed\n",
    "downloads** up to 10 times. After the final attempt, it will continue\n",
    "regardless of whether some files failed. You can re-run this cell to try\n",
    "downloading any remaining files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tcga_dir = analysis_dir / \"tcga_scc\"\n",
    "tcga_downloaded_tag = tcga_dir / \".download_complete\"\n",
    "tcga_metadata_dir_tar = analysis_dir / \"tcga_metadata.tar.gz\"\n",
    "tcga_metadata_dir = tcga_metadata_dir_tar.with_suffix(\"\").with_suffix(\"\")\n",
    "\n",
    "tcga_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "geo_metadata_url = \"https://raw.githubusercontent.com/brj0/mepylome-data/main/examples/geo_metadata.tar.gz\"\n",
    "tcga_metadata_url = \"https://raw.githubusercontent.com/brj0/mepylome-data/main/examples/tcga_metadata.tar.gz\"\n",
    "\n",
    "# Check if the TCGA annotation tar file exists and extract\n",
    "if not tcga_metadata_dir.exists():\n",
    "    print(\"Setting up TCGA annotation directory...\")\n",
    "    download_file(tcga_metadata_url, tcga_metadata_dir_tar)\n",
    "    extract_tar(tcga_metadata_dir_tar, analysis_dir)\n",
    "    print(\"Setting up TCGA annotation directory done.\")\n",
    "\n",
    "\n",
    "def download_tcga_file(file_id, filename, dest_folder):\n",
    "    \"\"\"Downloads a TCGA file safely, cleaning up incomplete downloads.\"\"\"\n",
    "    final_path = dest_folder / filename\n",
    "    temp_path = final_path.with_suffix(final_path.suffix + \".part\")\n",
    "    if final_path.exists():\n",
    "        print(f\"File already exists: {filename}\")\n",
    "        return True\n",
    "    url = f\"https://api.gdc.cancer.gov/data/{file_id}\"\n",
    "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            with temp_path.open(\"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        temp_path.rename(final_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {filename} | Error: {e}\")\n",
    "        if temp_path.exists():\n",
    "            temp_path.unlink()\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_all_tcga_files(manifest_path, dest_folder, max_threads):\n",
    "    \"\"\"Downloads all TCGA files in manifest in parallel.\"\"\"\n",
    "    manifest = pd.read_csv(manifest_path, sep=\"\\t\")\n",
    "    total_files = len(manifest)\n",
    "    pending = [\n",
    "        item\n",
    "        for _, item in manifest.iterrows()\n",
    "        if not (dest_folder / item[\"filename\"]).exists()\n",
    "    ]\n",
    "    attempt = 1\n",
    "    downloaded = total_files - len(pending)\n",
    "    MAX_ATTEMPTS = 10\n",
    "    lock = Lock()\n",
    "    while pending and attempt <= MAX_ATTEMPTS:\n",
    "        print(f\"\\n--- Attempt {attempt}: {len(pending)} files remaining ---\")\n",
    "        next_round = []\n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    download_tcga_file,\n",
    "                    item[\"id\"],\n",
    "                    item[\"filename\"],\n",
    "                    dest_folder,\n",
    "                ): item\n",
    "                for item in pending\n",
    "            }\n",
    "            for future in as_completed(futures):\n",
    "                item = futures[future]\n",
    "                success = future.result()\n",
    "                if success:\n",
    "                    with lock:\n",
    "                        downloaded += 1\n",
    "                    print(\n",
    "                        f\"Downloaded file {downloaded}/{total_files}: \"\n",
    "                        f\"{item['filename']}\"\n",
    "                    )\n",
    "                else:\n",
    "                    next_round.append(item)\n",
    "        pending = next_round\n",
    "        attempt += 1\n",
    "    if not pending:\n",
    "        print(\"\\n--- All files downloaded successfully ---\")\n",
    "        tcga_downloaded_tag.touch()\n",
    "    else:\n",
    "        print(\"\\n--- Some files failed to download. Retry limit reached. ---\")\n",
    "\n",
    "\n",
    "# Check if the download is complete\n",
    "if not tcga_downloaded_tag.exists():\n",
    "    print(\"Download has not been completed yet.\")\n",
    "    print(\"Downloading TCGA files. This may take several hours!\")\n",
    "    manifest_file = next(tcga_metadata_dir.glob(\"gdc_manifest.*txt\"))\n",
    "    if not manifest_file.exists():\n",
    "        msg = \"No TCGA manifest file found.\"\n",
    "        raise FileNotFoundError(msg)\n",
    "    print(f\"Downloading TCGA data from manifest file: {manifest_file}\")\n",
    "    download_all_tcga_files(manifest_file, tcga_dir, max_threads=8)\n",
    "else:\n",
    "    print(\"TCGA data already completely downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Clean up by removing array types other than `450k` and `epic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def remove_invalid_array_types(root_dir):\n",
    "    \"\"\"Removes all IDAT files that are not of type 450k or epicv1.\"\"\"\n",
    "    idat_files = root_dir.glob(\"*idat\")\n",
    "    valid_array_types = {ArrayType.ILLUMINA_450K, ArrayType.ILLUMINA_EPIC}\n",
    "    for idat_file in idat_files:\n",
    "        array_type = ArrayType.from_idat(idat_file)\n",
    "        if array_type not in valid_array_types:\n",
    "            print(f\"Removing {idat_file.name} (Type: {array_type})\")\n",
    "            idat_file.unlink()\n",
    "\n",
    "\n",
    "remove_invalid_array_types(tcga_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Next we extract the TCGA annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tcga_case_id_dict(json_path):\n",
    "    \"\"\"Extracts a dictionary mapping from IDAT IDs to case IDs.\"\"\"\n",
    "    with json_path.open() as f:\n",
    "        data = json.load(f)\n",
    "    case_id_mapping = {}\n",
    "    n_suffix = len(\"_Grn.idat\")\n",
    "    for item in data:\n",
    "        file_name = item.get(\"file_name\", \"\")[:-n_suffix]\n",
    "        case_id = item.get(\"associated_entities\", [{}])[0].get(\"case_id\", \"\")\n",
    "        if case_id and file_name:\n",
    "            case_id_mapping[case_id] = file_name\n",
    "    return case_id_mapping\n",
    "\n",
    "\n",
    "json_metadata = next(tcga_metadata_dir.glob(\"metadata.cart.*json\"))\n",
    "case_id_to_sample_id = extract_tcga_case_id_dict(json_metadata)\n",
    "\n",
    "# Load the clinical data and map the case_id to the IDAT\n",
    "tcga_annotation = pd.read_csv(\n",
    "    tcga_metadata_dir / \"clinical.tsv\", delimiter=\"\\t\"\n",
    ")\n",
    "tcga_annotation[\"Sample_ID\"] = tcga_annotation[\"case_id\"].map(\n",
    "    case_id_to_sample_id\n",
    ")\n",
    "tcga_annotation = tcga_annotation.drop(columns=\"case_id\")\n",
    "\n",
    "# Rename columns restrict to the useful ones\n",
    "columns_dict = {\n",
    "    \"gender\": \"Sex\",\n",
    "    \"age_at_index\": \"Age\",\n",
    "    \"tissue_or_organ_of_origin\": \"Tumor_site\",\n",
    "    \"site_of_resection_or_biopsy\": \"Site_of_resection_or_biopsy\",\n",
    "    \"tumor_grade\": \"Tumor_grade\",\n",
    "    \"morphology\": \"Morphology\",\n",
    "    \"primary_diagnosis\": \"Primary_diagnosis\",\n",
    "    \"Sample_ID\": \"Sample_ID\",\n",
    "}\n",
    "tcga_annotation = tcga_annotation.rename(columns=columns_dict)\n",
    "tcga_annotation = tcga_annotation[columns_dict.values()]\n",
    "\n",
    "# Standardize the 'Sex' column and convert 'Age' to numeric\n",
    "tcga_annotation[\"Sex\"] = tcga_annotation[\"Sex\"].replace(\n",
    "    {\"female\": \"Female\", \"male\": \"Male\"}\n",
    ")\n",
    "tcga_annotation[\"Age\"] = pd.to_numeric(tcga_annotation[\"Age\"], errors=\"coerce\")\n",
    "\n",
    "# Mark the samples that to be censored\n",
    "diag_to_censor_stat = {\n",
    "    \"Adenocarcinoma with mixed subtypes\": 0,\n",
    "    \"Adenocarcinoma, NOS\": 0,\n",
    "    \"Adenosquamous carcinoma\": 1,\n",
    "    \"Basaloid squamous cell carcinoma\": 1,\n",
    "    \"Lymphoepithelial carcinoma\": 1,\n",
    "    \"Papillary carcinoma, NOS\": 1,\n",
    "    \"Papillary squamous cell carcinoma\": 1,\n",
    "    \"Squamous cell carcinoma, NOS\": 0,\n",
    "    \"Squamous cell carcinoma, keratinizing, NOS\": 0,\n",
    "    \"Squamous cell carcinoma, large cell, nonkeratinizing, NOS\": 1,\n",
    "    \"Squamous cell carcinoma, nonkeratinizing, NOS\": 0,\n",
    "    \"Squamous cell carcinoma, small cell, nonkeratinizing\": 1,\n",
    "    \"Squamous cell carcinoma, spindle cell\": 1,\n",
    "    \"Warty carcinoma\": 1,\n",
    "}\n",
    "\n",
    "tcga_annotation[\"Censor\"] = tcga_annotation[\"Primary_diagnosis\"].map(\n",
    "    diag_to_censor_stat\n",
    ")\n",
    "\n",
    "# Condense the primary tumor site.\n",
    "nsclc_sites = {\n",
    "    \"Lower lobe, lung\",\n",
    "    \"Lung, NOS\",\n",
    "    \"Main bronchus\",\n",
    "    \"Middle lobe, lung\",\n",
    "    \"Overlapping lesion of lung\",\n",
    "    \"Upper lobe, lung\",\n",
    "}\n",
    "hnsq_sites = {\n",
    "    \"Anterior floor of mouth\",\n",
    "    \"Base of tongue, NOS\",\n",
    "    \"Border of tongue\",\n",
    "    \"Cheek mucosa\",\n",
    "    \"Floor of mouth, NOS\",\n",
    "    \"Gum, NOS\",\n",
    "    \"Hard palate\",\n",
    "    \"Head, face or neck, NOS\",\n",
    "    \"Hypopharynx, NOS\",\n",
    "    \"Larynx, NOS\",\n",
    "    \"Lip, NOS\",\n",
    "    \"Lower gum\",\n",
    "    \"Mandible\",\n",
    "    \"Mouth, NOS\",\n",
    "    \"Nasal cavity\",\n",
    "    \"Oropharynx, NOS\",\n",
    "    \"Overlapping lesion of lip, oral cavity and pharynx\",\n",
    "    \"Palate, NOS\",\n",
    "    \"Pharynx, NOS\",\n",
    "    \"Posterior wall of oropharynx\",\n",
    "    \"Retromolar area\",\n",
    "    \"Supraglottis\",\n",
    "    \"Tongue, NOS\",\n",
    "    \"Tonsil, NOS\",\n",
    "    \"Upper Gum\",\n",
    "    \"Ventral surface of tongue, NOS\",\n",
    "}\n",
    "cervix_sites = {\"Cervix uteri\"}\n",
    "eso_sites = {\n",
    "    \"Cardia, NOS\",\n",
    "    \"Esophagus, NOS\",\n",
    "    \"Lower third of esophagus\",\n",
    "    \"Middle third of esophagus\",\n",
    "    \"Thoracic esophagus\",\n",
    "    \"Upper third of esophagus\",\n",
    "}\n",
    "censor_sites = {\n",
    "    \"Breast, NOS\",\n",
    "    \"Bladder, NOS\",\n",
    "}\n",
    "\n",
    "tcga_annotation[\"Diagnosis\"] = None\n",
    "\n",
    "# Classify each row based on the tumor site\n",
    "for index, row in tcga_annotation.iterrows():\n",
    "    site = str(row[\"Tumor_site\"]).strip()\n",
    "    diagnosis = row[\"Primary_diagnosis\"]\n",
    "    if diagnosis.startswith(\"Adenocarcinoma\") and site in nsclc_sites:\n",
    "        tcga_annotation.loc[index, \"Diagnosis\"] = \"NSCLC_AD\"\n",
    "    elif site in cervix_sites:\n",
    "        tcga_annotation.loc[index, \"Diagnosis\"] = \"CERSQ_CA\"\n",
    "    elif site in nsclc_sites:\n",
    "        tcga_annotation.loc[index, \"Diagnosis\"] = \"NSCLC_SC\"\n",
    "    elif site in hnsq_sites:\n",
    "        tcga_annotation.loc[index, \"Diagnosis\"] = \"HNSQ_CA\"\n",
    "    elif site in eso_sites:\n",
    "        tcga_annotation.loc[index, \"Diagnosis\"] = \"ESO_CA_SQ\"\n",
    "    else:\n",
    "        tcga_annotation.loc[index, \"Censor\"] = 1\n",
    "        print(f\"Unmatched tumor site: {site} (index {index} - censored)\")\n",
    "\n",
    "# Removed censored samples\n",
    "tcga_annotation = tcga_annotation[tcga_annotation[\"Censor\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Step 2: Download and unzip the GEO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_metadata_dir_tar = analysis_dir / \"geo_metadata.tar.gz\"\n",
    "geo_metadata_dir = geo_metadata_dir_tar.with_suffix(\"\").with_suffix(\"\")\n",
    "\n",
    "# Check if the GEO annotation tar file exists and extract\n",
    "if not geo_metadata_dir.exists():\n",
    "    print(\"Setting up GEO annotation directory...\")\n",
    "    download_file(geo_metadata_url, geo_metadata_dir_tar)\n",
    "    extract_tar(geo_metadata_dir_tar, analysis_dir)\n",
    "    print(\"Setting up GEO annotation directory done.\")\n",
    "\n",
    "# Download the IDAT files.\n",
    "download_from_geo_and_untar(analysis_dir, datasets[tumor_site][\"geo_ids\"])\n",
    "download_from_geo_and_untar(\n",
    "    test_dir, datasets[tumor_site + \"_test\"][\"geo_ids\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Download the annotation spreadsheet.\n",
    "def merge_csv(dir_path):\n",
    "    \"\"\"Reads all CSV files merges them.\"\"\"\n",
    "    dir_path = Path(dir_path)\n",
    "    merged_df = pd.DataFrame()\n",
    "    for csv_file in dir_path.glob(\"*.csv\"):\n",
    "        print(f\"Reading {csv_file}\")\n",
    "        data_frame = pd.read_csv(csv_file)\n",
    "        merged_df = pd.concat([merged_df, data_frame], ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "geo_annotation = merge_csv(geo_metadata_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Step 3: Construct the annotation file of all data.\n",
    "Join the TCGA and GEO annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if (csv_path := analysis_dir / f\"{tumor_site}.csv\").exists():\n",
    "    anno_df = pd.read_csv(csv_path)\n",
    "    print(\"Merged annotation file allready exists.\")\n",
    "else:\n",
    "    anno_df = pd.concat([geo_annotation, tcga_annotation], ignore_index=True)\n",
    "    # Remove Adenocarcinoma and normal oral cavity samples\n",
    "    scc_types = {\"HNSQ_CA\", \"NSCLC_SC\", \"ESO_CA_SQ\", \"CERSQ_CA\"}\n",
    "    anno_df = anno_df[anno_df[\"Diagnosis\"].isin(scc_types)]\n",
    "    anno_df.to_csv(csv_path, index=False)\n",
    "    print(\"Merged annotation file created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Create the Methylation Analysis Object\n",
    "\n",
    "The `MethylAnalysis` object serves as the main interface for performing DNA\n",
    "methylation analysis. Key parameters such as the directory structure, number\n",
    "of CpG sites, and UMAP settings are configured here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider test files with annotation.\n",
    "test_ids = set(anno_df.Sample_ID).intersection(\n",
    "    x.name for x in idat_basepaths(test_dir)\n",
    ")\n",
    "\n",
    "analysis = MethylAnalysis(\n",
    "    analysis_dir=analysis_dir,\n",
    "    reference_dir=reference_dir,\n",
    "    output_dir=output_dir,\n",
    "    test_dir=test_dir,\n",
    "    test_ids=test_ids,\n",
    "    n_cpgs=25000,\n",
    "    load_full_betas=True,\n",
    "    overlap=True,\n",
    "    cpg_blacklist=blacklist,\n",
    "    cpgs=\"450k+epic+epicv2\",\n",
    "    debug=False,\n",
    "    do_seg=True,\n",
    "    umap_parms={\n",
    "        \"n_neighbors\": 5,\n",
    "        \"metric\": \"manhattan\",\n",
    "        \"min_dist\": 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Load Beta Values\n",
    "\n",
    "Reads and processes beta values from the provided dataset. This step can also\n",
    "be performed interactively within the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis.set_betas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"2.-UMAP-Calculation\"></a>\n",
    "## 2. UMAP Calculation\n",
    "\n",
    "### Generate UMAP Plot\n",
    "\n",
    "Set the columns used for coloring the UMAP plot before initiating the\n",
    "dimensionality reduction process. The UMAP algorithm produces a visual\n",
    "representation of the sample clusters, which is stored as a Plotly object in\n",
    "`analysis.umap_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate UMAP\n",
    "analysis.idat_handler.selected_columns = [\"Diagnosis\"]\n",
    "analysis.make_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "print(analysis.umap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate and show image\n",
    "output_path = output_dir / f\"{analysis_dir.name}_umap_plot.jpg\"\n",
    "analysis.umap_plot.write_image(\n",
    "    output_path,\n",
    "    format=\"jpg\",\n",
    "    width=IMG_HEIGHT,\n",
    "    height=IMG_WIDTH,\n",
    "    scale=1,\n",
    ")\n",
    "IPImage(filename=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Launch the Analysis GUI\n",
    "\n",
    "Initializes an interactive GUI for further exploration of the methylation\n",
    "data.\n",
    "\n",
    "**Note:** This step works best in local environments and may have limitations\n",
    "on platforms like Google Colab or Binder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis.run_app(open_tab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "On memory-limited platforms such as Google Colab, we need to manually free up\n",
    "memory between operations to avoid crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"3.-Supervised-Classifier-Training\"></a>\n",
    "## 3. Supervised Classifier Training\n",
    "\n",
    "### Supervised Classifier Validation\n",
    "\n",
    "The next step involves validating various supervised classification\n",
    "algorithms to evaluate their performance on the dataset. This process helps\n",
    "identify the most accurate model for methylation-based classification.\n",
    "\n",
    "**Note**:\n",
    "Training is resource- and time-intensive. The process may take up to 10\n",
    "minutes, depending on the computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "***Preselect features (optional, recommended for low-memory systems)***:\n",
    "To reduce memory usage during training, we preselect the top 25,000 most\n",
    "variable CpG sites (used for UMAP above) as the feature matrix. This\n",
    "dramatically lowers RAM requirements. If memory is not a concern, you can\n",
    "remove this line to include all CpGs in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis.feature_matrix = analysis.betas_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train supervised classifiers\n",
    "\n",
    "analysis.idat_handler.selected_columns = [\"Diagnosis\"]\n",
    "ids = analysis.idat_handler.ids\n",
    "clf_out = analysis.classify(\n",
    "    ids=ids,\n",
    "    clf_list=[\n",
    "        \"vtl-kbest(k=10000)-et\",\n",
    "        \"vtl-kbest(k=10000)-lr(max_iter=10000)\",\n",
    "        \"vtl-kbest(k=10000)-rf\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print reports for all classifier for the first sample\n",
    "for clf_result in clf_out:\n",
    "    print(clf_result.reports[\"txt\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and display the best classifier\n",
    "best_clf = max(\n",
    "    clf_out, key=lambda result: np.mean(result.metrics[\"accuracy_scores\"])\n",
    ")\n",
    "print(\"Most accurate classifier:\")\n",
    "print(best_clf.reports[\"txt\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Now we apply the best classifier on the independent validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = analysis.idat_handler.test_ids\n",
    "# Ignore all files that are not in annotation file\n",
    "test_ids = list(\n",
    "    set(test_ids).intersection(analysis.idat_handler.annotation_df.index)\n",
    ")\n",
    "clf_out_pred = analysis.classify(ids=test_ids, clf_list=best_clf.model)\n",
    "pred = clf_out_pred[0].prediction.idxmax(axis=1)\n",
    "true_values = analysis.idat_handler.samples_annotated.loc[test_ids][\n",
    "    \"Diagnosis\"\n",
    "]\n",
    "correct = np.sum(pred == true_values)\n",
    "total = len(pred)\n",
    "accuracy = correct / total\n",
    "print(f\"Classifier Accuracy: {100 * accuracy:.2f} % ({correct}/{total})\")\n",
    "\n",
    "# Analyze misclassified samples\n",
    "misclassified = pred != true_values\n",
    "misclassified_samples = clf_out_pred[0].prediction[misclassified].copy()\n",
    "misclassified_samples[\"Pred\"] = pred[misclassified]\n",
    "misclassified_samples[\"True\"] = true_values[misclassified]\n",
    "print(\"Missclassified samples:\\n\", misclassified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "-----------------------------------------------------------------------------\n",
    "<a name=\"4.-CNV-Analysis\"></a>\n",
    "## 4. CNV Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Generate and Save CNV Plot\n",
    "\n",
    "Creates a copy number variation (CNV) plot for a specified sample and saves\n",
    "the output as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CNV example\n",
    "analysis.make_cnv_plot(\"GSM3519735_202915460126_R05C01\")\n",
    "cnv_plot = analysis.cnv_plot\n",
    "cnv_plot.update_layout(\n",
    "    yaxis_range=[-1.1, 1.1],\n",
    "    font={\"size\": FONTSIZE},\n",
    "    margin={\"t\": 50},\n",
    ")\n",
    "output_path = output_dir / f\"{analysis_dir.name}_cnv_plot.jpg\"\n",
    "cnv_plot.write_image(\n",
    "    output_path,\n",
    "    format=\"jpg\",\n",
    "    width=IMG_HEIGHT,\n",
    "    height=IMG_WIDTH,\n",
    "    scale=1,\n",
    ")\n",
    "IPImage(filename=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Generate CNV Summary Plots\n",
    "\n",
    "In addition to individual CNV plots, this step computes summary plots to\n",
    "highlight genomic alterations across multiple samples.\n",
    "\n",
    "**Note**:\n",
    "Generating all copy number variation (CNV) plots is resource- and\n",
    "time-intensive. The process can take a significant amount of time, depending\n",
    "on the computational resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.precompute_cnvs()\n",
    "cn_summary_path = calculate_cn_summary(analysis, \"Diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "IPImage(filename=cn_summary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "\n",
    "## Appendix\n",
    "\n",
    "The GEO annotation files were downloaded and manually curated. Below is the\n",
    "code used to download the GEO datasets and save their associated metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install geoparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GEOparse\n",
    "\n",
    "geo_numbers = datasets[\"scc\"][\"geo_ids\"] + datasets[\"scc_test\"][\"geo_ids\"]\n",
    "\n",
    "# Download and save metadata for each GEO series\n",
    "for geo_nr in geo_numbers:\n",
    "    gse = GEOparse.get_GEO(geo=geo_nr, destdir=geo_metadata_dir)\n",
    "    metadata = gse.phenotype_data\n",
    "    print(metadata.head())\n",
    "    metadata.to_csv(\n",
    "        geo_metadata_dir / f\"{geo_nr}_raw_metadata.csv\", index=True\n",
    "    )\n",
    "\n",
    "for file in geo_metadata_dir.glob(\"*.soft.gz\"):\n",
    "    print(f\"Removing file: {file}\")\n",
    "    file.unlink()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
